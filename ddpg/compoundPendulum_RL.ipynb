{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pendulum Swing-up Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=false; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taken from https://stackoverflow.com/questions/31517194/how-to-hide-one-specific-cell-input-or-output-in-ipython-notebook\n",
    "tag = HTML('''<script>\n",
    "code_show=false; \n",
    "function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    } else {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "display(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: centre;\" src=\"compoundPendulum.png\" alt=\"Compound Pendulum\" width=\"200\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure shows a compound pendulum of mass *m*. Let $\\hat{i}$, $\\hat{j}$ and $\\hat{\\lambda}$ be the unit vectors in the respective directions shown, $dx$ is an elemental length at a distance $x$ from the pivot point. A torque of $u$ is applied on the pivot and $L$ is the length of the pendulum. The mass per unit length is $m/L$ and the mass of $dx$ will be $\\frac{m}{L}dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Equation of motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ r = x \\hat{\\lambda}$$ where $\\hat{\\lambda} =  \\sin\\theta \\hat{i} - \\cos\\theta \\hat{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{r} &= x \\dot{\\lambda} \\\\ \n",
    "    &= x(\\dot{\\theta} \\hat{k} \\times \\lambda) \\\\\n",
    "    &= x \\dot{\\theta} \\hat{n}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kinectic Energy, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    T &= \\frac{1}{2}\\int_0^l \\dot{r}~.~\\dot{r} dm \\\\\n",
    "      &= \\frac{1}{2}\\int_0^l x^2 \\dot{\\theta}^2 dx \\frac{m}{L} \\\\\n",
    "      &= \\frac{1}{6} m l^2 \\dot{\\theta}^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U = -mg\\frac{l}{2}\\cos\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    L &= T - U \\\\\n",
    "      &= \\frac{1}{6} ml^2 \\dot{\\theta}^2 + mg\\frac{l}{2}\\cos\\theta\n",
    "       \\frac{\\partial L}{\\partial \\theta} &= -mg\\frac{l}{2}\\sin\\theta \\\\\n",
    "    \\frac{\\partial L}{\\partial \\dot{\\theta}} &= \\frac{1}{3}ml^2\\dot{\\theta}^2 \\\\\n",
    "    \\frac{d}{dt}\\Big(\\frac{\\partial L}{\\partial \\dot{\\theta}}\\Big) &= \\frac{1}{3}ml^2\\ddot{\\theta} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Equation of motion becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{d}{dt}\\Big(\\frac{\\partial L}{\\partial \\dot{\\theta}}\\Big) - \\frac{\\partial L}{\\partial \\theta} &= u \\\\\n",
    "    \\frac{1}{3}ml^2\\ddot{\\theta} + mg\\frac{l}{2}\\sin\\theta &= u \\\\\n",
    "    \\ddot{\\theta} + \\frac{3g}{2l} \\sin\\theta &= \\frac{3u}{ml^2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if there is a damping term, then the equation of motion becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ \\ddot{\\theta} + c\\dot{\\theta} +\\frac{3g}{2l} \\sin\\theta = \\frac{3u}{ml^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From finite difference approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\dot{\\theta}_{t+1} - \\dot{\\theta}_{t}}{dt} &= -\\frac{3g}{2l} \\sin\\theta + \\frac{3u}{ml^2}\\\\\n",
    "    \\dot{\\theta}_{t+1} &= \\dot{\\theta}_{t} + dt \\Big(-\\frac{3g}{2l} \\sin\\theta + \\frac{3u}{ml^2}\\Big)\\\\\n",
    "    \\theta_{t+1} &= \\theta_t + dt ~ \\dot{\\theta}_{t+1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Openai Gym pendulum environment](https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardDynamics(self, u):\n",
    "    th, thdot = self.state  # th := theta\n",
    "\n",
    "    g = self.g  # gravity = 9.81\n",
    "    m = self.m  # mass\n",
    "    l = self.l  # length\n",
    "    dt = self.dt # time increment\n",
    "\n",
    "    u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "    \n",
    "    newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt\n",
    "    newth = th + newthdot * dt\n",
    "    newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "     \n",
    "    self.state = np.array([newth, newthdot])\n",
    "    return newth, newthdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is carried out for neural networks to learn faster. These are observations.\n",
    "def _get_obs(self):\n",
    "    theta, thetadot = self.state\n",
    "    return np.array([np.cos(theta), np.sin(theta), thetadot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum reward that you can obtain is 0. At either side of the vertical position, the reward goes negative. So the aim is to hold the pendulum vertical which means the angle $\\theta$ and angular velocity $\\dot{\\theta}$ should both be 0 with minimal control effort\n",
    "\n",
    "$$R = r_t(s_t, a_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_normalize(x):  # to keep angles in the range -pi to pi\n",
    "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n",
    "\n",
    "# When pendulum is vertical, theta = 0.\n",
    "def reward(self, u):\n",
    "    theta, thetadot = self.state\n",
    "    reward = -(angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2))\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other reward functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: centre;\" src=\"rewards.png\" alt=\"\" width=\"300\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=false; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tag)\n",
    "# reward = -(s_t - s_T)**2 + u.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done and info are useful for other RL environments but is included \n",
    "# here to have a consistency. Usually done in this context means if the pendulum \n",
    "# vertical or not (always boolean). Info corresponds to any useful info corresponding\n",
    "# to the environment that you want to return if there exists\n",
    "\n",
    "def step(self, u):\n",
    "    obs = self._get_obs()\n",
    "    reward = self.reward(u)\n",
    "    done = False  \n",
    "    info = {}\n",
    "    return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL cannot find a solution to the problem in the first run itself. It requires a \n",
    "# lot of explorations for get something meaningful. So after exploring for sometime\n",
    "# it has to be set back to some initial configurations. Reset() function does that\n",
    "\n",
    "def reset(self):\n",
    "    high = np.array([np.pi, 1])\n",
    "    self.state = self.np_random.uniform(low=-high, high=high)\n",
    "    self.last_u = None\n",
    "    return self._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(self):\n",
    "    # Write functions to visualize()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Therefore the basic components of an RL environment are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotEnv:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def _get_obs_(self, ):\n",
    "        pass\n",
    "    \n",
    "    def step(self, u):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex system, hand coding of forward dynamics and visualization are very difficult. Therefore simulators are used. Most popular simulators are Mujoco, pyBullet, ROS-Gazebo etc. These simulators need a robot description file, either a URDF or xml (for Mujoco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Openai gym environments](https://gym.openai.com/envs/#classic_control) are very popular for RL tasks. \n",
    "\n",
    "```pip install gym ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')  # gym.make('env_name-version')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim of RL: To maximize sum of expected future rewards by discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=false; \n",
       "function code_toggle() {\n",
       "    if (code_show){\n",
       "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "    } else {\n",
       "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "    }\n",
       "    code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "To show/hide this cell's raw code input, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tag) # to hide/show this cell\n",
    "idx = 500\n",
    "cum_rewards = np.zeros(500)\n",
    "env.reset()\n",
    "for i in range(idx):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    cum_rewards[i] = reward\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTF2",
   "language": "python",
   "name": "venvtf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
